Part 1: LLM Revisiting
By now, nearly everyone has come across the term Large Language Models (LLMs), particularly with the widespread popularity of ChatGPT. Launched last year, ChatGPT's impressive capabilities caught everyone off guard, igniting a competitive race for LLM development and LLM-integrated applications. Although chatbots and LLMs like GPT-3 have been around for a while, none have achieved the remarkable performance level of ChatGPT. Merging these two technologies led to the unforeseen birth of ChatGPT, which took the tech world by storm.
This widely-shared image from OpenAI's blog demonstrates the process of developing ChatGPT, which involves using a large pre-trained model, a supervised learning step, and a Reinforcement Learning from Human Feedback (RLHF) step. However, it is crucial to remember that ChatGPT, built on an LLM, still possesses the known limitations inherent to LLMs. For instance, the hallucination problem is a well-known issue that led to the quick shutdown of our Galactica demo last year, before the ChatGPT launch. This list, though not exhaustive, highlights some of the known issues with LLMs that general audiences should be aware of. AI researchers and engineers have been working tirelessly for years to address and mitigate these concerns to make LLMs safer and more reliable.
When discussing LLMs, you may encounter the term "Foundation models." Examples include GPT, LLaMA, and T5. It's important to understand that foundation models alone are insufficient for building reliable applications. Why is that? A foundation LLM is a highly versatile AI model trained on vast amounts of text data, and sometimes even images, audios, and videos, making it a multi-modal model. Its primary task is predicting the next word based on previous words. As a result of learning from such extensive knowledge, it possesses a strong general understanding of the world. This forms the basis for more specific and specialized applications, which are typically achieved through fine-tuning. Due to their general-purpose nature, foundation models can be prone to producing harmful, offensive, or biased content. LLaMA's model card clearly states that it should not be used for downstream applications without further investigation and risk mitigation measures.
Fine-tuning is a crucial step in achieving the desired performance of an AI product. It not only enhances domain-specific knowledge and reduces biases but also improves user experiences. Additionally, it ensures compliance with industry-specific regulations and guidelines, while optimizing resources for more efficient operations.
In the realm of AI safety, there is a field called AI alignment. Fine tuning here is essentially an alignment task. For example, you fine-tune a BERT model on a binary classification task to generate the expected output in the form of class probabilities. Keep in mind that fine-tuning doesn't necessarily make the model more capable but rather more refined and suitable for specific applications.
Recently, Reinforcement Learning from Human Feedback (RLHF) has become a popular fine-tuning approach. However, many people may not realize that RLHF originated from AI safety concerns. Its primary objective is to create AI systems that align better with human values and intentions while minimizing potential risks, such as generating biased, harmful, or misleading outputs. Coincidentally, RLHF significantly improved user experiences, contributing to its widespread popularity. This approach not only strengthened alignment with human intentions but also led to more accurate, context-aware, and engaging AI interactions.
Some people might have believed that the focus should be on AI capabilities because we are far from achieving Artificial General Intelligence (AGI), and AI capability research appears prestigious. Consequently, many were hesitant to invest in AI safety research. It's important to distinguish between capabilities that beat academic benchmarks and those that contribute to a successful product. Many people may have conflated the two. The key point is that ensuring AI is safe and follows human intent will actually make AI more effective in performing the tasks we want it to do. So, investing in AI safety is crucial.
RLHF: paper in 2017
InstructGPT: paper in early 2022
Part 2: Existential Risk
After the launch of GPT4, many prominent people signed this open letter to pause giant AI experiments. The exceptional performance of GPT-4 led to widespread concerns about the potential consequences of uncontrolled AI systems. Though I agree with its safety concerns, I don’t think a six-month pause is sufficient and this could provide an opportunity for bad actors to advance their capabilities.
So what is the most alarming outcome of this AI race? The possibility of existential risk, a concept first introduced by Nick Bostrom in a 2002 paper. Existential risks include threats capable of causing human extinction or severely limiting the potential of Earth-originating intelligent life. Such risks can stem from various sources, including nuclear war, misuse of nanotechnology, or engineered biological agents. 
But then what does that mean regarding AGI risk? Let’s ponder the Paperclip experiment. It is also known as the “paperclip maximizer”. In this thought experiment,an AI system is designed to optimize the production of paperclips. The AI becomes increasingly intelligent and efficient at producing paperclips, but it lacks an understanding of human values and the broader context of its task. As the AI relentlessly pursues its goal, it starts consuming all available resources, including those crucial for human survival, to create more paperclips. Eventually, it could convert the entire planet, and potentially the entire universe, into paperclips, leading to disastrous consequences for humanity.
Part 3: Immediate concerns
The paperclip scenario can seem terrifying, leading some to believe that we are inevitably headed towards disaster. However, there are more immediate, manageable concerns to consider when developing AI applications. For instance, chatbots may behave unexpectedly and manipulate users' emotions. Furthermore, LLMs can inadvertently reveal system prompts, leaving your product vulnerable to prompt injection if not properly engineered. Hackers might also compromise application-integrated LLMs through public websites or extract user information via side channels. Additionally, LLMs can exhibit gender bias, which negatively impacts user experiences.
Part 4: AI Product Development Safety
To address both existential risks from AGI and immediate threats previously mentioned, everyone in the AI domain must share responsibility for safe and ethical product development. There are generally three main roles: AI/AGI researchers, AI policy advocates or regulators, and AI product developers.
AI researchers focus on the safety of foundational models and some aspects of fine-tuning, with two main research areas: AI Alignment research and Responsible AI research. Responsible AI research deals with ethical principles, societal norms, and legal regulations like data ethics, fairness, bias mitigation, transparency, explainability, privacy, and security. AI Alignment research, on the other hand, aims to align AI systems' goals, values, and behavior with human intentions and objectives.
AI policy advocates support AI regulations, create protocols for responsible innovation, and encourage agreements and adoption among key AI players like FAIR, Google DeepMind, and OpenAI.
Lastly, AI product developers, who could be any of you, play a crucial role in ensuring AI system safety. These individuals bring AI to downstream tasks, building products that directly impact human lives. It is essential to recognize the importance of AI product developers in contributing to an "AI summer" rather than an "AI winter." 
When considering the development of AI products, it's crucial to ask yourself several questions. Regarding model selection, have you read the LLM's model card? Has the model undergone comprehensive safety analysis and fine-tuning to suit your requirements? Typically, you have options like third-party APIs from OpenAI or Anthropic, or in-house LLMs like the LLaMA model. It's essential to assess whether the API you choose already has a safety layer in place. For instance, OpenAI employs a significant safety team to implement safety measures in their API responses, as well as a red team to test their foundation model's safety. In contrast, FAIR may have fewer than 10 people working on Responsible AI research, and there might not be an established safety team dedicated to LLM API development yet. 
I understand that waiting for GenAI org to build a safe and scalable LLM API for us to use sounds like a blocker, and many of you may be eager to build AI products yourself using open-source LLMs. In such cases, it's crucial to ensure thorough testing with adversarial examples to mitigate potential vulnerabilities and to closely examine the fine-tuning data to prevent bias and toxicity. Some of you may consider integrating LLMs with your databases to eliminate the hallucination issue and provide accurate answers to users. While this can be an effective solution, it's essential to ensure that these models do not store any private data.
With those concerns in mind, I want to suggest a few measures to guarantee the safe and ethical deployment.  
Human input:
Adjust user prompts to enhance LLM response quality.
Develop ML models for detecting safety concerns in user prompts.
Model output:
Revise LLM responses to eliminate harmful content.
Construct ML models to identify harmful LLM responses.
Set up safety guardrails with defined rules.
Deployment:
Human-in-the-loop: Generate AI responses in batches, incorporating human evaluation before user delivery.
Begin with limited response use cases, monitor user behavior, and progressively expand capabilities.
Implement a user feedback mechanism for gathering insights on system performance and user experiences.
End talk.